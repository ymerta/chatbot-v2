#!/usr/bin/env python3
"""
Test script to quickly verify the accuracy improvements
"""

import asyncio
import os
from pathlib import Path

def test_basic_functionality():
    """Test basic chatbot functionality with improvements"""
    print("ğŸ§ª Testing Basic Chatbot Functionality")
    print("=" * 50)
    
    try:
        # Test imports
        from src.graph.app_graph import build_app_graph
        from src.config import FAISS_STORE_PATH
        from langchain_community.vectorstores import FAISS
        from langchain_openai import OpenAIEmbeddings
        
        print("âœ… All imports successful")
        
        # Check if FAISS store exists
        if not Path(FAISS_STORE_PATH).exists():
            print("âŒ FAISS store not found. Run: python src/index_build.py")
            return False
        
        print("âœ… FAISS store found")
        
        # Load chatbot
        emb = OpenAIEmbeddings()
        vs = FAISS.load_local(FAISS_STORE_PATH, emb, allow_dangerous_deserialization=True)
        docs = vs.docstore._dict
        
        corpus_texts = [d.page_content for d in docs.values()]
        corpus_meta = [d.metadata for d in docs.values()]
        
        graph = build_app_graph(corpus_texts, corpus_meta)
        print("âœ… Chatbot graph compiled successfully")
        
        # Test queries
        test_queries = [
            "Netmera'da push notification nasÄ±l gÃ¶nderilir?",
            "Android SDK integration steps",
            "How to create user segments in Netmera?"
        ]
        
        print("\nğŸ” Testing sample queries:")
        for i, query in enumerate(test_queries, 1):
            try:
                result = graph.invoke({"query": query})
                answer = result.get("answer", "")
                
                print(f"\n{i}. Query: {query}")
                print(f"   Answer length: {len(answer)} chars")
                print(f"   Has structured format: {'**AdÄ±mlar:**' in answer or '1.' in answer}")
                print(f"   Has technical terms: {any(term in answer.lower() for term in ['sdk', 'api', 'push', 'campaign'])}")
                print(f"   Has helpful indicators: {'ğŸ’¡' in answer or 'âš ï¸' in answer}")
                
                if len(answer) > 100:
                    print(f"   Preview: {answer[:100]}...")
                
            except Exception as e:
                print(f"âŒ Query {i} failed: {e}")
                return False
        
        print("\nâœ… Basic functionality test completed")
        return True
        
    except Exception as e:
        print(f"âŒ Basic test failed: {e}")
        return False


async def test_evaluation_system():
    """Test the evaluation system"""
    print("\nğŸ§ª Testing Evaluation System")
    print("=" * 50)
    
    try:
        from src.evaluation import NetmeraEvaluator
        from src.evaluation.langsmith_evaluator import (
            accuracy_evaluator, 
            helpfulness_evaluator,
            completeness_evaluator,
            language_consistency_evaluator
        )
        
        print("âœ… Evaluation imports successful")
        
        # Test evaluators with sample data
        sample_run = type('Run', (), {
            'outputs': {
                'answer': """Netmera'da push notification gÃ¶ndermek iÃ§in ÅŸu adÄ±mlarÄ± takip edin:

**AdÄ±mlar:**
1. Netmera dashboard'a giriÅŸ yapÄ±n
2. Sol menÃ¼den 'Campaigns' bÃ¶lÃ¼mÃ¼ne tÄ±klayÄ±n
3. 'Push Notifications' seÃ§eneÄŸini seÃ§in
4. 'Create Campaign' butonuna tÄ±klayÄ±n

ğŸ’¡ **Ä°pucu:** Test gÃ¶nderimi yaparak kampanyanÄ±zÄ± kontrol edin
âš ï¸ **Dikkat:** Hedef kitleyi doÄŸru seÃ§tiÄŸinizden emin olun

**Ã–rnek kod:**
```java
implementation 'com.netmera:netmera-android:3.x.x'
```"""
            }
        })()
        
        sample_example = type('Example', (), {})()
        
        # Test each evaluator
        evaluators = [
            ("Accuracy", accuracy_evaluator),
            ("Helpfulness", helpfulness_evaluator), 
            ("Completeness", completeness_evaluator),
            ("Language Consistency", language_consistency_evaluator)
        ]
        
        print("\nğŸ“Š Testing evaluators:")
        for name, evaluator_func in evaluators:
            try:
                result = evaluator_func(sample_run, sample_example)
                score = result.get('score', 0)
                reason = result.get('reason', 'No reason')
                
                print(f"   {name}: {score:.3f} - {reason}")
                
            except Exception as e:
                print(f"âŒ {name} evaluator failed: {e}")
                return False
        
        print("\nâœ… Evaluation system test completed")
        return True
        
    except Exception as e:
        print(f"âŒ Evaluation test failed: {e}")
        return False


def check_environment():
    """Check environment setup"""
    print("\nğŸ§ª Checking Environment")
    print("=" * 50)
    
    checks = [
        ("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY")),
        ("LANGSMITH_API_KEY", os.getenv("LANGSMITH_API_KEY")),
        ("FAISS Store", Path("data/embeddings/faiss_store").exists()),
        ("Evaluation DB", Path("EvaluationDB").exists()),
    ]
    
    all_good = True
    for name, status in checks:
        if status:
            print(f"âœ… {name}")
        else:
            print(f"âš ï¸  {name} - Not configured/found")
            if name.endswith("_API_KEY"):
                all_good = False
    
    return all_good


def suggest_next_steps():
    """Suggest next steps based on test results"""
    print("\nğŸ’¡ Recommended Next Steps")
    print("=" * 50)
    
    print("""
1. ğŸš€ Run Full Evaluation:
   python evaluate_chatbot.py --dataset all

2. ğŸ“Š Compare Results:
   python -m src.evaluation.reporting --recent 1

3. ğŸ”„ Iterate Based on Results:
   - Check accuracy scores in LangSmith dashboard
   - Identify low-scoring examples
   - Further tune system prompt or evaluators

4. ğŸ“ˆ Monitor Performance:
   - Set up regular evaluations
   - Track improvements over time
   - A/B test different prompts

5. ğŸ› ï¸ Advanced Improvements:
   - Query preprocessing for Turkish
   - Custom fine-tuning
   - Enhanced retrieval strategies
""")


async def main():
    """Main test function"""
    print("ğŸ¯ NETMERA CHATBOT ACCURACY IMPROVEMENTS TEST")
    print("=" * 60)
    
    # Environment check
    env_ok = check_environment()
    
    # Basic functionality test
    basic_ok = test_basic_functionality()
    
    # Evaluation system test
    eval_ok = await test_evaluation_system()
    
    # Summary
    print("\nğŸ“‹ TEST SUMMARY")
    print("=" * 30)
    print(f"Environment: {'âœ…' if env_ok else 'âš ï¸'}")
    print(f"Basic Functionality: {'âœ…' if basic_ok else 'âŒ'}")
    print(f"Evaluation System: {'âœ…' if eval_ok else 'âŒ'}")
    
    if all([basic_ok, eval_ok]):
        print("\nğŸ‰ All core tests passed!")
        if not env_ok:
            print("âš ï¸  Some environment variables missing but system functional")
    else:
        print("\nâŒ Some tests failed - check errors above")
    
    suggest_next_steps()
    
    return all([basic_ok, eval_ok])


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)

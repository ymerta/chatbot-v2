# ğŸ¤– Netmera Chatbot LangSmith Evaluation System

Bu dokÃ¼man, Netmera chatbot'unun performansÄ±nÄ± LangSmith kullanarak nasÄ±l deÄŸerlendireceÄŸinizi aÃ§Ä±klar.

## ğŸ“‹ Ä°Ã§indekiler

- [Kurulum](#kurulum)
- [HÄ±zlÄ± BaÅŸlangÄ±Ã§](#hÄ±zlÄ±-baÅŸlangÄ±Ã§)
- [Dataset'ler](#datasetler)
- [Evaluation Metrikleri](#evaluation-metrikleri)
- [KullanÄ±m Ã–rnekleri](#kullanÄ±m-Ã¶rnekleri)
- [Reporting](#reporting)
- [Troubleshooting](#troubleshooting)

## ğŸš€ Kurulum

### 1. Gerekli Dependencies

```bash
pip install -r requirements.txt
```

### 2. Environment Variables

`evaluation.env.example` dosyasÄ±nÄ± `evaluation.env` olarak kopyalayÄ±n ve gerÃ§ek deÄŸerleri girin:

```bash
cp evaluation.env.example evaluation.env
```

Gerekli environment variables:

```bash
# LangSmith Configuration (Zorunlu)
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=netmera-chatbot-evaluation

# OpenAI Configuration (Zorunlu)
OPENAI_API_KEY=your_openai_api_key_here

# Optional Settings
CHAT_MODEL=gpt-4o
EMBEDDING_MODEL=text-embedding-3-large
```

### 3. LangSmith API Key AlÄ±n

1. [LangSmith](https://smith.langchain.com/) hesabÄ± oluÅŸturun
2. API key'inizi alÄ±n: Settings â†’ API Keys â†’ Create API Key
3. Environment variable'a ekleyin

## âš¡ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Tek Komutla Evaluation

```bash
# TÃ¼m dataset'leri evaluate et
python evaluate_chatbot.py --dataset all

# Belirli bir dataset'i evaluate et
python evaluate_chatbot.py --dataset netmera-basic-qa

# Ã–zel experiment adÄ±yla
python evaluate_chatbot.py --dataset netmera-advanced-qa --experiment my_test_run
```

### Python Kodu ile

```python
from src.evaluation import NetmeraEvaluator, run_evaluation
import asyncio

# Evaluator'Ä± initialize et
evaluator = NetmeraEvaluator()

# Dataset'leri oluÅŸtur
evaluator.create_dataset(
    "my-test-dataset",
    "netmera_chatbot_dataset.json", 
    "Test dataset"
)

# Evaluation'Ä± Ã§alÄ±ÅŸtÄ±r
results = asyncio.run(run_evaluation(
    evaluator, 
    "my-test-dataset", 
    "my_experiment"
))
```

## ğŸ“Š Dataset'ler

Sistem, `EvaluationDB/` klasÃ¶rÃ¼ndeki mevcut dataset'leri kullanÄ±r:

### 1. netmera-basic-qa
- **Dosya**: `netmera_chatbot_dataset.json`
- **Ä°Ã§erik**: Temel Netmera Ã¶zellikleri ve kullanÄ±m senaryolarÄ±
- **Ã–rnekler**: ~20 soru-cevap Ã§ifti
- **Kategoriler**: genel_bilgi, push_notifications, sdk_integration, user_segmentation

### 2. netmera-advanced-qa  
- **Dosya**: `netmera_extended_dataset.json`
- **Ä°Ã§erik**: GeliÅŸmiÅŸ teknik konular ve troubleshooting
- **Ã–rnekler**: ~20 teknik soru-cevap Ã§ifti
- **Kategoriler**: troubleshooting, api_integration, ab_testing, personalization

### 3. netmera-chat-scenarios
- **Dosya**: `netmera_chat_dataset.json`  
- **Ä°Ã§erik**: Multi-turn konuÅŸma senaryolarÄ±
- **Ã–rnekler**: ~5 gerÃ§ek chat senaryosu
- **Kategoriler**: support, technical_support, guidance, educational

## ğŸ¯ Evaluation Metrikleri

### 1. Accuracy (DoÄŸruluk) - %30 aÄŸÄ±rlÄ±k
- Teknik doÄŸruluk ve Netmera terminolojisi
- Netmera terimlerinin doÄŸru kullanÄ±mÄ±
- Kod Ã¶rneklerinin doÄŸruluÄŸu
- **Hedef**: â‰¥0.7

### 2. Completeness (KapsamlÄ±lÄ±k) - %25 aÄŸÄ±rlÄ±k  
- CevaplarÄ±n kapsamlÄ±lÄ±ÄŸÄ± ve detay seviyesi
- AdÄ±m adÄ±m aÃ§Ä±klamalar
- Ã–rnekler ve alternatifler
- **Hedef**: â‰¥0.6

### 3. Helpfulness (FaydalÄ±lÄ±k) - %25 aÄŸÄ±rlÄ±k
- KullanÄ±cÄ± iÃ§in pratik deÄŸer
- Actionable tavsiyeler  
- Problem Ã§Ã¶zme odaklÄ±lÄ±k
- **Hedef**: â‰¥0.6

### 4. Language Consistency (Dil TutarlÄ±lÄ±ÄŸÄ±) - %20 aÄŸÄ±rlÄ±k
- TÃ¼rkÃ§e/Ä°ngilizce tutarlÄ±lÄ±ÄŸÄ±
- KarÄ±ÅŸÄ±k dil kullanÄ±mÄ± tespiti
- **Hedef**: â‰¥0.8

## ğŸ’» KullanÄ±m Ã–rnekleri

### Ã–rnek 1: Temel Evaluation

```bash
# Config'i kontrol et
python -c "from src.evaluation.config import print_config_status; print_config_status()"

# Dataset'leri listele
python evaluate_chatbot.py --list-datasets

# Temel evaluation
python evaluate_chatbot.py --dataset netmera-basic-qa
```

### Ã–rnek 2: Comprehensive Analysis

```bash
# TÃ¼m dataset'lerde evaluation
python evaluate_chatbot.py --dataset all --experiment comprehensive_test

# Report oluÅŸtur
python -m src.evaluation.reporting --recent 1
```

### Ã–rnek 3: Custom Evaluation

```python
from src.evaluation import NetmeraEvaluator
from src.evaluation.reporting import EvaluationReporter

# Custom evaluator
evaluator = NetmeraEvaluator()

# Yeni dataset oluÅŸtur  
dataset_id = evaluator.create_dataset(
    "custom-netmera-qa",
    "my_custom_dataset.json",
    "Custom evaluation dataset"
)

# Evaluation Ã§alÄ±ÅŸtÄ±r
import asyncio
results = asyncio.run(run_evaluation(
    evaluator,
    "custom-netmera-qa", 
    "custom_experiment"
))

# Report oluÅŸtur
reporter = EvaluationReporter()
reports = reporter.generate_comprehensive_report(["custom_experiment"])
```

## ğŸ“ˆ Reporting

### Otomatik Report Generation

```bash
# Son 7 gÃ¼nÃ¼n experiment'leri iÃ§in report
python -m src.evaluation.reporting --recent 7

# Belirli experiment'ler iÃ§in
python -m src.evaluation.reporting --experiments exp1 exp2 exp3

# Custom output
python -m src.evaluation.reporting --experiments my_exp --output custom_report
```

### Report FormatlarÄ±

1. **HTML Report** (interactive)
   - Performance charts
   - Detailed metrics table  
   - Recommendations
   - Benchmark comparisons

2. **CSV Export** (data analysis)
   - Raw scores per example
   - Timestamp tracking
   - Easy pivot/analysis

3. **JSON Export** (programmatic)
   - Machine-readable format
   - API integration
   - Custom processing

### Report Lokasyonu

Reports ÅŸu klasÃ¶rde oluÅŸturulur:
```
./evaluation_reports/
â”œâ”€â”€ evaluation_report_20231211_143022.html
â”œâ”€â”€ evaluation_report_20231211_143022.csv  
â”œâ”€â”€ evaluation_report_20231211_143022.json
â”œâ”€â”€ evaluation_scores_20231211_143022.png
â””â”€â”€ dataset_comparison_20231211_143022.png
```

## ğŸ”§ Advanced Usage

### Custom Evaluators

```python
from langsmith.schemas import Run, Example

def custom_netmera_evaluator(run: Run, example: Example) -> dict:
    """Custom evaluation logic"""
    prediction = run.outputs.get("answer", "")
    
    # Custom scoring logic
    score = 0.0
    if "netmera" in prediction.lower():
        score += 0.5
    if any(term in prediction for term in ["push", "campaign", "segment"]):
        score += 0.3
    
    return {
        "key": "custom_metric",
        "score": score,
        "reason": "Custom evaluation criteria"
    }

# Add to evaluation
from langsmith.evaluation import evaluate

evaluate(
    predictor_func,
    data="dataset_name",
    evaluators=[custom_netmera_evaluator],
    experiment_prefix="custom_eval"
)
```

### Batch Processing

```python
import asyncio
from src.evaluation import NetmeraEvaluator, run_evaluation

async def batch_evaluation():
    evaluator = NetmeraEvaluator()
    
    datasets = ["netmera-basic-qa", "netmera-advanced-qa", "netmera-chat-scenarios"]
    
    tasks = []
    for dataset in datasets:
        task = run_evaluation(evaluator, dataset, f"batch_{dataset}")
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results

# Run batch evaluation
results = asyncio.run(batch_evaluation())
```

### Performance Monitoring

```python
from src.evaluation.reporting import EvaluationReporter

reporter = EvaluationReporter()

# Son 30 gÃ¼nÃ¼n trendleri
experiments = reporter.get_recent_experiments(30)
summaries = [reporter.analyze_experiment(exp) for exp in experiments]

# Performance trend analysis
for summary in summaries:
    print(f"{summary.experiment_name}: Accuracy={summary.average_scores.get('accuracy', 0):.3f}")
```

## ğŸ” Benchmark'lar ve Hedefler

### Performance Tier'larÄ±

| Metric | Excellent | Good | Acceptable | Poor |
|--------|-----------|------|------------|------|
| Accuracy | â‰¥0.90 | â‰¥0.75 | â‰¥0.60 | <0.60 |
| Completeness | â‰¥0.85 | â‰¥0.70 | â‰¥0.55 | <0.55 |
| Helpfulness | â‰¥0.80 | â‰¥0.65 | â‰¥0.50 | <0.50 |
| Language Consistency | â‰¥0.95 | â‰¥0.85 | â‰¥0.70 | <0.70 |

### Response Time Benchmarks

| Category | Target | Good | Acceptable |
|----------|--------|------|------------|
| Simple Q&A | <2s | <5s | <10s |
| Complex Technical | <5s | <8s | <15s |  
| Multi-turn Chat | <3s | <6s | <12s |

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. LANGSMITH_API_KEY bulunamÄ±yor
```bash
export LANGSMITH_API_KEY="your_key_here"
# veya evaluation.env dosyasÄ±nda set edin
```

#### 2. Dataset dosyasÄ± bulunamÄ±yor
```bash
# EvaluationDB klasÃ¶rÃ¼nÃ¼ kontrol edin
ls -la EvaluationDB/
```

#### 3. FAISS store yÃ¼kleme hatasÄ±
```bash
# Data klasÃ¶rÃ¼nÃ¼ kontrol edin
ls -la data/embeddings/faiss_store/
```

#### 4. OpenAI API rate limiting
```
# Model'i deÄŸiÅŸtirin (config.py)
CHAT_MODEL = "gpt-3.5-turbo"  # instead of gpt-4o
```

### Debug Mode

```python
import logging

# Debug logging aktif et
logging.basicConfig(level=logging.DEBUG)

# Evaluator'da verbose mode
evaluator = NetmeraEvaluator()
evaluator.debug = True
```

### Configuration Check

```bash
python -c "
from src.evaluation.config import validate_config, print_config_status
errors = validate_config()
if errors:
    print('âŒ Configuration errors:')
    for error in errors: print(f'  - {error}')
else:
    print('âœ… Configuration is valid')
print_config_status()
"
```

## ğŸ“š API Reference

### NetmeraEvaluator

```python
class NetmeraEvaluator:
    def __init__(self, api_key=None, project_name="netmera-chatbot-evaluation")
    def create_dataset(self, dataset_name: str, dataset_file: str, description: str) -> str
    def list_datasets(self) -> List[Dict]
    def chatbot_predictor(self, inputs: Dict[str, Any]) -> Dict[str, Any]
```

### EvaluationReporter

```python
class EvaluationReporter:
    def __init__(self, langsmith_client=None)
    def analyze_experiment(self, experiment_name: str) -> EvaluationSummary
    def generate_html_report(self, summaries: List[EvaluationSummary]) -> Path
    def generate_comprehensive_report(self, experiment_names: List[str]) -> Dict[str, Path]
```

## ğŸ¤ KatkÄ±da Bulunma

### Yeni Evaluator Ekleme

1. `src/evaluation/langsmith_evaluator.py`'da yeni evaluator function tanÄ±mlayÄ±n
2. `EVALUATOR_CONFIGS`'e configuration ekleyin  
3. Test edin ve dokÃ¼mante edin

### Yeni Dataset Ekleme

1. `EvaluationDB/` klasÃ¶rÃ¼ne JSON dosyasÄ± ekleyin
2. `src/evaluation/config.py`'da `DATASET_CONFIGS`'e ekleyin
3. Schema'nÄ±n doÄŸru olduÄŸunu kontrol edin

### Performance Ä°yileÅŸtirmeleri  

1. Async/await pattern'lerini kullanÄ±n
2. Batch processing'i optimize edin
3. Caching stratejileri implementasyonlarÄ±

## ğŸ“ Destek

- **GitHub Issues**: Sorunlar ve Ã¶zellik istekleri iÃ§in
- **Documentation**: Bu README ve kod dokÃ¼mantasyonu
- **LangSmith Docs**: https://docs.smith.langchain.com/

## ğŸ“„ Lisans

Bu evaluation sistemi MIT lisansÄ± altÄ±nda sunulmuÅŸtur.

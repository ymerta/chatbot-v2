# ü§ñ Netmera Chatbot LangSmith Evaluation System

Bu dok√ºman, Netmera chatbot'unun performansƒ±nƒ± LangSmith kullanarak nasƒ±l deƒüerlendireceƒüinizi a√ßƒ±klar.

## üìã ƒ∞√ßindekiler

- [Kurulum](#kurulum)
- [Hƒ±zlƒ± Ba≈ülangƒ±√ß](#hƒ±zlƒ±-ba≈ülangƒ±√ß)
- [Dataset'ler](#datasetler)
- [Evaluation Metrikleri](#evaluation-metrikleri)
- [Kullanƒ±m √ñrnekleri](#kullanƒ±m-√∂rnekleri)
- [Reporting](#reporting)
- [Troubleshooting](#troubleshooting)

## üöÄ Kurulum

### 1. Gerekli Dependencies

```bash
pip install -r requirements.txt
```

### 2. Environment Variables

`evaluation.env.example` dosyasƒ±nƒ± `evaluation.env` olarak kopyalayƒ±n ve ger√ßek deƒüerleri girin:

```bash
cp evaluation.env.example evaluation.env
```

Gerekli environment variables:

```bash
# LangSmith Configuration (Zorunlu)
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=netmera-chatbot-evaluation

# OpenAI Configuration (Zorunlu)
OPENAI_API_KEY=your_openai_api_key_here

# Optional Settings
CHAT_MODEL=gpt-4o
EMBEDDING_MODEL=text-embedding-3-large
```

### 3. LangSmith API Key Alƒ±n

1. [LangSmith](https://smith.langchain.com/) hesabƒ± olu≈üturun
2. API key'inizi alƒ±n: Settings ‚Üí API Keys ‚Üí Create API Key
3. Environment variable'a ekleyin

## ‚ö° Hƒ±zlƒ± Ba≈ülangƒ±√ß

### Tek Komutla Evaluation

```bash
# T√ºm dataset'leri evaluate et
python evaluate_chatbot.py --dataset all

# Belirli bir dataset'i evaluate et
python evaluate_chatbot.py --dataset netmera-basic-qa

# √ñzel experiment adƒ±yla
python evaluate_chatbot.py --dataset netmera-advanced-qa --experiment my_test_run
```

### Python Kodu ile

```python
from src.evaluation import NetmeraEvaluator, run_evaluation
import asyncio

# Evaluator'ƒ± initialize et
evaluator = NetmeraEvaluator()

# Dataset'leri olu≈ütur
evaluator.create_dataset(
    "my-test-dataset",
    "netmera_chatbot_dataset.json", 
    "Test dataset"
)

# Evaluation'ƒ± √ßalƒ±≈ütƒ±r
results = asyncio.run(run_evaluation(
    evaluator, 
    "my-test-dataset", 
    "my_experiment"
))
```

## üìä Dataset'ler

Sistem, `EvaluationDB/` klas√∂r√ºndeki mevcut dataset'leri kullanƒ±r:

### 1. netmera-basic-qa
- **Dosya**: `netmera_chatbot_dataset.json`
- **ƒ∞√ßerik**: Temel Netmera √∂zellikleri ve kullanƒ±m senaryolarƒ±
- **√ñrnekler**: ~20 soru-cevap √ßifti
- **Kategoriler**: genel_bilgi, push_notifications, sdk_integration, user_segmentation

### 2. netmera-advanced-qa  
- **Dosya**: `netmera_extended_dataset.json`
- **ƒ∞√ßerik**: Geli≈ümi≈ü teknik konular ve troubleshooting
- **√ñrnekler**: ~20 teknik soru-cevap √ßifti
- **Kategoriler**: troubleshooting, api_integration, ab_testing, personalization

### 3. netmera-chat-scenarios
- **Dosya**: `netmera_chat_dataset.json`  
- **ƒ∞√ßerik**: Multi-turn konu≈üma senaryolarƒ±
- **√ñrnekler**: ~5 ger√ßek chat senaryosu
- **Kategoriler**: support, technical_support, guidance, educational

## üéØ Evaluation Metrikleri

### 1. Accuracy (Doƒüruluk) - %30 aƒüƒ±rlƒ±k
- Teknik doƒüruluk ve Netmera terminolojisi
- Netmera terimlerinin doƒüru kullanƒ±mƒ±
- Kod √∂rneklerinin doƒüruluƒüu
- **Hedef**: ‚â•0.7

### 2. Completeness (Kapsamlƒ±lƒ±k) - %25 aƒüƒ±rlƒ±k  
- Cevaplarƒ±n kapsamlƒ±lƒ±ƒüƒ± ve detay seviyesi
- Adƒ±m adƒ±m a√ßƒ±klamalar
- √ñrnekler ve alternatifler
- **Hedef**: ‚â•0.6

### 3. Helpfulness (Faydalƒ±lƒ±k) - %25 aƒüƒ±rlƒ±k
- Kullanƒ±cƒ± i√ßin pratik deƒüer
- Actionable tavsiyeler  
- Problem √ß√∂zme odaklƒ±lƒ±k
- **Hedef**: ‚â•0.6

### 4. Language Consistency (Dil Tutarlƒ±lƒ±ƒüƒ±) - %20 aƒüƒ±rlƒ±k
- T√ºrk√ße/ƒ∞ngilizce tutarlƒ±lƒ±ƒüƒ±
- Karƒ±≈üƒ±k dil kullanƒ±mƒ± tespiti
- **Hedef**: ‚â•0.8

## üíª Kullanƒ±m √ñrnekleri

### √ñrnek 1: Temel Evaluation

```bash
# Config'i kontrol et
python -c "from src.evaluation.config import print_config_status; print_config_status()"

# Dataset'leri listele
python evaluate_chatbot.py --list-datasets

# Temel evaluation
python evaluate_chatbot.py --dataset netmera-basic-qa
```

### √ñrnek 2: Comprehensive Analysis

```bash
# T√ºm dataset'lerde evaluation
python evaluate_chatbot.py --dataset all --experiment comprehensive_test

# Report olu≈ütur
python -m src.evaluation.reporting --recent 1
```

### √ñrnek 3: Custom Evaluation

```python
from src.evaluation import NetmeraEvaluator
from src.evaluation.reporting import EvaluationReporter

# Custom evaluator
evaluator = NetmeraEvaluator()

# Yeni dataset olu≈ütur  
dataset_id = evaluator.create_dataset(
    "custom-netmera-qa",
    "my_custom_dataset.json",
    "Custom evaluation dataset"
)

# Evaluation √ßalƒ±≈ütƒ±r
import asyncio
results = asyncio.run(run_evaluation(
    evaluator,
    "custom-netmera-qa", 
    "custom_experiment"
))

# Report olu≈ütur
reporter = EvaluationReporter()
reports = reporter.generate_comprehensive_report(["custom_experiment"])
```

## üìà Reporting

### Otomatik Report Generation

```bash
# Son 7 g√ºn√ºn experiment'leri i√ßin report
python -m src.evaluation.reporting --recent 7

# Belirli experiment'ler i√ßin
python -m src.evaluation.reporting --experiments exp1 exp2 exp3

# Custom output
python -m src.evaluation.reporting --experiments my_exp --output custom_report
```

### Report Formatlarƒ±

1. **HTML Report** (interactive)
   - Performance charts
   - Detailed metrics table  
   - Recommendations
   - Benchmark comparisons

2. **CSV Export** (data analysis)
   - Raw scores per example
   - Timestamp tracking
   - Easy pivot/analysis

3. **JSON Export** (programmatic)
   - Machine-readable format
   - API integration
   - Custom processing

### Report Lokasyonu

Reports ≈üu klas√∂rde olu≈üturulur:
```
./evaluation_reports/
‚îú‚îÄ‚îÄ evaluation_report_20231211_143022.html
‚îú‚îÄ‚îÄ evaluation_report_20231211_143022.csv  
‚îú‚îÄ‚îÄ evaluation_report_20231211_143022.json
‚îú‚îÄ‚îÄ evaluation_scores_20231211_143022.png
‚îî‚îÄ‚îÄ dataset_comparison_20231211_143022.png
```

## üîß Advanced Usage

### Custom Evaluators

```python
from langsmith.schemas import Run, Example

def custom_netmera_evaluator(run: Run, example: Example) -> dict:
    """Custom evaluation logic"""
    prediction = run.outputs.get("answer", "")
    
    # Custom scoring logic
    score = 0.0
    if "netmera" in prediction.lower():
        score += 0.5
    if any(term in prediction for term in ["push", "campaign", "segment"]):
        score += 0.3
    
    return {
        "key": "custom_metric",
        "score": score,
        "reason": "Custom evaluation criteria"
    }

# Add to evaluation
from langsmith.evaluation import evaluate

evaluate(
    predictor_func,
    data="dataset_name",
    evaluators=[custom_netmera_evaluator],
    experiment_prefix="custom_eval"
)
```

### Batch Processing

```python
import asyncio
from src.evaluation import NetmeraEvaluator, run_evaluation

async def batch_evaluation():
    evaluator = NetmeraEvaluator()
    
    datasets = ["netmera-basic-qa", "netmera-advanced-qa", "netmera-chat-scenarios"]
    
    tasks = []
    for dataset in datasets:
        task = run_evaluation(evaluator, dataset, f"batch_{dataset}")
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results

# Run batch evaluation
results = asyncio.run(batch_evaluation())
```

### Performance Monitoring

```python
from src.evaluation.reporting import EvaluationReporter

reporter = EvaluationReporter()

# Son 30 g√ºn√ºn trendleri
experiments = reporter.get_recent_experiments(30)
summaries = [reporter.analyze_experiment(exp) for exp in experiments]

# Performance trend analysis
for summary in summaries:
    print(f"{summary.experiment_name}: Accuracy={summary.average_scores.get('accuracy', 0):.3f}")
```

## üîç Benchmark'lar ve Hedefler

### Performance Tier'larƒ±

| Metric | Excellent | Good | Acceptable | Poor |
|--------|-----------|------|------------|------|
| Accuracy | ‚â•0.90 | ‚â•0.75 | ‚â•0.60 | <0.60 |
| Completeness | ‚â•0.85 | ‚â•0.70 | ‚â•0.55 | <0.55 |
| Helpfulness | ‚â•0.80 | ‚â•0.65 | ‚â•0.50 | <0.50 |
| Language Consistency | ‚â•0.95 | ‚â•0.85 | ‚â•0.70 | <0.70 |

### Response Time Benchmarks

| Category | Target | Good | Acceptable |
|----------|--------|------|------------|
| Simple Q&A | <2s | <5s | <10s |
| Complex Technical | <5s | <8s | <15s |  
| Multi-turn Chat | <3s | <6s | <12s |

## üîß Troubleshooting

### Common Issues

#### 1. LANGSMITH_API_KEY bulunamƒ±yor
```bash
export LANGSMITH_API_KEY="your_key_here"
# veya evaluation.env dosyasƒ±nda set edin
```

#### 2. Dataset dosyasƒ± bulunamƒ±yor
```bash
# EvaluationDB klas√∂r√ºn√º kontrol edin
ls -la EvaluationDB/
```

#### 3. FAISS store y√ºkleme hatasƒ±
```bash
# Data klas√∂r√ºn√º kontrol edin
ls -la data/embeddings/faiss_store/
```

#### 4. OpenAI API rate limiting
```
# Model'i deƒüi≈ütirin (config.py)
CHAT_MODEL = "gpt-3.5-turbo"  # instead of gpt-4o
```

### Debug Mode

```python
import logging

# Debug logging aktif et
logging.basicConfig(level=logging.DEBUG)

# Evaluator'da verbose mode
evaluator = NetmeraEvaluator()
evaluator.debug = True
```

### Configuration Check

```bash
python -c "
from src.evaluation.config import validate_config, print_config_status
errors = validate_config()
if errors:
    print('‚ùå Configuration errors:')
    for error in errors: print(f'  - {error}')
else:
    print('‚úÖ Configuration is valid')
print_config_status()
"
```

## üìö API Reference

### NetmeraEvaluator

```python
class NetmeraEvaluator:
    def __init__(self, api_key=None, project_name="netmera-chatbot-evaluation")
    def create_dataset(self, dataset_name: str, dataset_file: str, description: str) -> str
    def list_datasets(self) -> List[Dict]
    def chatbot_predictor(self, inputs: Dict[str, Any]) -> Dict[str, Any]
```

### EvaluationReporter

```python
class EvaluationReporter:
    def __init__(self, langsmith_client=None)
    def analyze_experiment(self, experiment_name: str) -> EvaluationSummary
    def generate_html_report(self, summaries: List[EvaluationSummary]) -> Path
    def generate_comprehensive_report(self, experiment_names: List[str]) -> Dict[str, Path]
```

## ü§ù Katkƒ±da Bulunma

### Yeni Evaluator Ekleme

1. `src/evaluation/langsmith_evaluator.py`'da yeni evaluator function tanƒ±mlayƒ±n
2. `EVALUATOR_CONFIGS`'e configuration ekleyin  
3. Test edin ve dok√ºmante edin

### Yeni Dataset Ekleme

1. `EvaluationDB/` klas√∂r√ºne JSON dosyasƒ± ekleyin
2. `src/evaluation/config.py`'da `DATASET_CONFIGS`'e ekleyin
3. Schema'nƒ±n doƒüru olduƒüunu kontrol edin

### Performance ƒ∞yile≈ütirmeleri  

1. Async/await pattern'lerini kullanƒ±n
2. Batch processing'i optimize edin
3. Caching stratejileri implementasyonlarƒ±

## üìû Destek

- **GitHub Issues**: Sorunlar ve √∂zellik istekleri i√ßin
- **Documentation**: Bu README ve kod dok√ºmantasyonu
- **LangSmith Docs**: https://docs.smith.langchain.com/

## üìÑ Lisans

Bu evaluation sistemi MIT lisansƒ± altƒ±nda sunulmu≈ütur.

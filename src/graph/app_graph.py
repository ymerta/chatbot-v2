try:
    from typing import TypedDict
except ImportError:
    from typing_extensions import TypedDict

from typing import List, Optional
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from src.config import CHAT_MODEL, SYSTEM_PROMPT
from src.retrievers.hybrid import HybridRetriever

class BotState(TypedDict, total=False):
    query: str
    lang: str
    translated_query: str
    docs: List[Document]            # burada string tutacaÄŸÄ±z (uyum iÃ§in)
    citations: List[str]
    answer: Optional[str]
    retrieval_conf: float

def detect_lang_and_passthrough(state: BotState) -> BotState:
    q = state["query"].strip()
    # Basit sezgi: TÃ¼rkÃ§e karakter iÃ§eriyorsa TR
    lang = "TÃ¼rkÃ§e" if any(ch in "Ã§ÄŸÄ±Ã¶ÅŸÃ¼" for ch in q.lower()) else "English"
    state.update({"lang": lang, "translated_query": q})
    return state

def detect_conversational_intent(state: BotState) -> BotState:
    """Conversational intent detection - disabled for direct Q&A only"""
    # Selamlama fonksiyonu devre dÄ±ÅŸÄ± - sadece sorulara cevap ver
    # state["conversational_response"] artÄ±k set edilmeyecek
    return state


def preprocess_query(query: str, lang: str) -> str:
    """Enhanced query preprocessing - Better Turkish-English mapping + error handling"""
    q_lower = query.lower()
    
    # Enhanced Turkish -> English term mapping
    if lang == "TÃ¼rkÃ§e":
        term_mapping = {
            # Existing terms
            "gÃ¼ncellenir": "update",
            "gÃ¼ncelleme": "update", 
            "kullanÄ±cÄ±": "user",
            "Ã¶zellik": "attribute",
            "Ã¶zellikler": "attributes",
            "nasÄ±l": "how",
            "kurulum": "install setup",
            "entegrasyon": "integration",
            "yapÄ±landÄ±rma": "configuration",
            "ayarlama": "setup configuration",
            "gÃ¶nderim": "send",
            "bildirim": "notification",
            "kampanya": "campaign",
            "segment": "segment",
            "analitik": "analytics",
            
            # ğŸ”§ NEW: Error and problem handling terms
            "hata": "error issue problem",
            "sorun": "problem issue trouble error",
            "limit": "limit size payload quota restriction",
            "boyut": "size payload limit length",
            "aÅŸtÄ±m": "exceed over limit maximum",
            "alÄ±yorum": "getting receiving encountering",
            
            # ğŸ”§ NEW: Network and access terms
            "ip": "ip address network connection",
            "adres": "address location endpoint",
            "adresim": "my address my ip address",
            "engel": "block blocked restrict ban",
            "engellenmiÅŸ": "blocked restricted banned",
            "yapabilirim": "can do solution fix resolve",
            
            # ğŸ”§ NEW: Technical and integration terms
            "modÃ¼l": "module component feature",
            "url": "url link endpoint address",
            "onay": "consent approval permission",
            "teslimat": "delivery send dispatch",
            "mesaj": "message notification push",
            "push": "push notification alert",
            "e-posta": "email mail electronic mail",
            "email": "email e-mail mail messaging"
        }
        
        # Enhanced query expansion
        enhanced_query = query
        for tr_term, en_term in term_mapping.items():
            if tr_term in q_lower:
                enhanced_query += f" {en_term}"
        
        # ğŸ”§ NEW: Pattern-based expansion for common issues
        if "push" in q_lower and ("boyut" in q_lower or "limit" in q_lower):
            enhanced_query += " push notification payload size maximum length restriction"
        
        if "ip" in q_lower and ("engel" in q_lower or "block" in q_lower):
            enhanced_query += " ip address blocked network access denied whitelist firewall"
            
    else:
        enhanced_query = query
        
        # ğŸ”§ NEW: English query expansion for technical terms
        if "integration" in q_lower and "module" in q_lower:
            enhanced_query += " platform integration setup configuration api"
        
        if "email" in q_lower and "delivery" in q_lower:
            enhanced_query += " email sending mail delivery smtp configuration"
    
    return enhanced_query

def retrieve_node(retriever: HybridRetriever):
    def _inner(state: BotState) -> BotState:
        q = state["translated_query"]
        lang = state["lang"]
        
        # Preprocess query to add English terms
        enhanced_q = preprocess_query(q, lang)
        
        items = retriever.retrieve(enhanced_q, k=10)
        state["docs"] = items
        
        # Better confidence calculation based on actual scores
        if items:
            max_score = max(item.get("score", 0) for item in items)
            # Normalize score to 0-1 range (scores can be > 1)
            conf = min(max_score / 1.5, 1.0)  # Lowered divisor for better confidence
        else:
            conf = 0.0
            
        state["retrieval_conf"] = conf
        return state
    return _inner

def decide_node(state: BotState) -> BotState:
    return state  # routing fonksiyonlarÄ± ayrÄ±

def generate_answer_node(llm: ChatOpenAI):
    def _inner(state: BotState) -> BotState:
        lang = state["lang"]
        q = state["translated_query"]
        docs = state.get("docs", [])
        
        # Enhanced context formatting
        formatted_contexts = []
        for i, doc in enumerate(docs[:3], 1):  # Limit to top 3 most relevant
            text = doc.get("text", "")
            source = doc.get("url", "unknown")
            
            # Preserve code formatting if present
            if any(indicator in text.lower() for indicator in ["gradle", "json", "xml", "implementation", "```", "curl"]):
                text = text.replace("\n\n", "\n").strip()  # Clean up extra newlines
            
            formatted_contexts.append(f"=== KAYNAK {i} ===\n{text}\n(URL: {source})\n")
        
        ctx = "\n".join(formatted_contexts)
        sys = SYSTEM_PROMPT
        
        # ğŸ”§ SIMPLIFIED: No forced formatting based on question type
        if lang == "TÃ¼rkÃ§e":
            prompt = f"""Soru: {q}

Belgeler:
{ctx}

YukarÄ±daki belgeleri kullanarak TÃœRKÃ‡E cevap verin. Gerekirse adÄ±mlar halinde aÃ§Ä±klayÄ±n."""
        else:
            prompt = f"""Question: {q}

Documentation:
{ctx}

Answer in ENGLISH using the documentation above. Use steps if needed."""

        out = llm.invoke([{"role":"system","content":sys},{"role":"user","content":prompt}]).content
        
        # Post-process the answer
        out = post_process_answer(out, lang)

        # SADECE Ä°LK URL
        primary = next((d.get("url") for d in docs if d.get("url")), None)
        state["citations"] = [primary] if primary else []
        state["answer"] = out
        return state
    return _inner

# ğŸ”§ REMOVED: is_procedural_question function - was causing accuracy issues


def post_process_answer(answer: str, lang: str) -> str:
    """Answer'Ä± post-process et - minimal formatting"""
    # Just clean up and return - no emoji additions
    return answer.strip()

def needs_clarification_check(state: BotState) -> BotState:
    """Sorunun aÃ§Ä±klayÄ±cÄ± soru gerekip gerekmediÄŸini kontrol et (SIMPLIFIED)"""
    query = state["translated_query"].lower()
    
    # Sadece Ã§ok belirsiz sorular iÃ§in clarification iste
    very_ambiguous = [
        len(query.split()) < 3,  # Ã‡ok kÄ±sa sorular
        query in ["help", "yardÄ±m", "nasÄ±l", "how"]  # Tek kelime sorular
    ]
    
    state["needs_clarification"] = any(very_ambiguous)
    return state

def clarify_question_node(llm: ChatOpenAI):
    """AÃ§Ä±klayÄ±cÄ± soru oluÅŸturur"""
    def _inner(state: BotState) -> BotState:
        lang = state["lang"]
        query = state["translated_query"]
        
        # AÃ§Ä±klayÄ±cÄ± soru prompt'u
        if lang == "TÃ¼rkÃ§e":
            clarify_prompt = f"""
KullanÄ±cÄ±nÄ±n sorusu: "{query}"

Bu soru belirsiz bilgiler iÃ§eriyor. KullanÄ±cÄ±ya daha iyi yardÄ±m edebilmek iÃ§in aÃ§Ä±klayÄ±cÄ± bir soru sor.

AÃ§Ä±klayÄ±cÄ± soru tÃ¼rleri:
- Platform/SDK: "Hangi platform iÃ§in? (iOS/Android/Web/React Native)"
- Versiyon: "Hangi Android Studio versiyonu kullanÄ±yorsunuz?"
- Dil: "Kotlin mi Java ile mi geliÅŸtiriyorsunuz?"
- Detay: "Hangi hata mesajÄ±nÄ± gÃ¶rÃ¼yorsunuz?"
- Ortam: "Development mi production ortamÄ±nda?"

KÄ±sa ve net bir aÃ§Ä±klayÄ±cÄ± soru Ã¼ret:
"""
        else:
            clarify_prompt = f"""
User's question: "{query}"

This question contains ambiguous information. Ask a clarifying question to better help the user.

Clarifying question types:
- Platform/SDK: "Which platform? (iOS/Android/Web/React Native)"
- Version: "Which Android Studio version are you using?"
- Language: "Are you using Kotlin or Java?"
- Details: "What error message do you see?"
- Environment: "Development or production environment?"

Generate a short and clear clarifying question:
"""
        
        # LLM'den aÃ§Ä±klayÄ±cÄ± soru al
        response = llm.invoke([
            {"role": "system", "content": "Sen yardÄ±mcÄ± bir asistansÄ±n. KullanÄ±cÄ±ya aÃ§Ä±klayÄ±cÄ± sorular sorarak daha iyi yardÄ±m ediyorsun."},
            {"role": "user", "content": clarify_prompt}
        ]).content.strip()
        
        state["clarifying_question"] = response
        state["answer"] = response  # UI iÃ§in
        return state
    return _inner


def finalize_node(state: BotState) -> BotState:
    # Finalize iÅŸlemi - artÄ±k conversational response yok
    return state

def route_after_retrieve(state: BotState) -> str:
    """Retrieval'dan sonra nereye gideceÄŸini belirle - HER ZAMAN GENERATE"""
    # Clarification Ã¶zelliÄŸi kaldÄ±rÄ±ldÄ± - her zaman direkt cevap ver
    return "generate"

# route_after_clarification_check fonksiyonu kaldÄ±rÄ±ldÄ± - artÄ±k kullanÄ±lmÄ±yor

def build_app_graph(corpus_texts, corpus_meta):
    llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)
    retriever = HybridRetriever(corpus_texts, corpus_meta)

    g = StateGraph(BotState)
    
    # Node'larÄ± ekle (clarification node'larÄ± kaldÄ±rÄ±ldÄ±)
    g.add_node("detect", detect_lang_and_passthrough)
    g.add_node("retrieve", retrieve_node(retriever))
    g.add_node("generate", generate_answer_node(llm))
    g.add_node("finalize", finalize_node)

    # BasitleÅŸtirilmiÅŸ graph flow - clarification kaldÄ±rÄ±ldÄ±
    g.set_entry_point("detect")
    g.add_edge("detect", "retrieve")
    g.add_edge("retrieve", "generate")  # Direkt generate'e git
    g.add_edge("generate", "finalize")

    return g.compile()
#!/usr/bin/env python3
"""
Netmera Chatbot Evaluation Demo Script
Bu script, evaluation sisteminin nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± gÃ¶sterir.
"""

import os
import sys
import asyncio
from pathlib import Path

def demo_basic_usage():
    """Temel kullanÄ±m demo'su"""
    print("ğŸ“š DEMO 1: Temel Evaluation KullanÄ±mÄ±")
    print("=" * 50)
    
    print("""
# 1. Environment setup kontrolÃ¼
python setup_evaluation.py

# 2. Mevcut dataset'leri listele  
python evaluate_chatbot.py --list-datasets

# 3. TÃ¼rkÃ§e Developer Guide dataset evaluation
python evaluate_chatbot.py --dataset netmera-developer-guide

# 4. TÃ¼rkÃ§e User guide dataset evaluation
python evaluate_chatbot.py --dataset netmera-user-guide

# 5. Ä°ngilizce Developer Guide dataset evaluation
python evaluate_chatbot.py --dataset netmera-developer-guide-en

# 6. Ä°ngilizce User Guide dataset evaluation
python evaluate_chatbot.py --dataset netmera-user-guide-en

# 7. TÃ¼m dataset'leri evaluate et (TÃ¼rkÃ§e + Ä°ngilizce)
python evaluate_chatbot.py --dataset all

# 8. Custom experiment adÄ±yla
python evaluate_chatbot.py --dataset netmera-developer-guide --experiment my_test
""")


def demo_python_api():
    """Python API kullanÄ±m demo'su"""
    print("\nğŸ“š DEMO 2: Python API KullanÄ±mÄ±")
    print("=" * 50)
    
    print("""
from src.evaluation import NetmeraEvaluator, run_evaluation
import asyncio

# Evaluator oluÅŸtur
evaluator = NetmeraEvaluator(project_name="my-evaluation-project")

# Mevcut dataset'leri listele
datasets = evaluator.list_datasets()
print(f"Mevcut dataset sayÄ±sÄ±: {len(datasets)}")

# Yeni dataset oluÅŸtur
dataset_id = evaluator.create_dataset(
    "my-custom-dataset",
    "netmera_developer_guide_dataset.json", 
    "Ã–zel test dataset'i"
)

# Evaluation Ã§alÄ±ÅŸtÄ±r
async def run_my_evaluation():
    results = await run_evaluation(
        evaluator,
        "my-custom-dataset",
        "custom_experiment_20231211"
    )
    return results

# Async evaluation Ã§alÄ±ÅŸtÄ±r
results = asyncio.run(run_my_evaluation())
""")


def demo_reporting():
    """Reporting demo'su"""
    print("\nğŸ“š DEMO 3: Reporting ve Analiz")
    print("=" * 50)
    
    print("""
from src.evaluation.reporting import EvaluationReporter

# Reporter oluÅŸtur
reporter = EvaluationReporter()

# Son 7 gÃ¼nÃ¼n experiment'lerini getir
recent_experiments = reporter.get_recent_experiments(7)
print(f"Son 7 gÃ¼nde {len(recent_experiments)} experiment yapÄ±ldÄ±")

# Belirli bir experiment'Ä± analiz et
summary = reporter.analyze_experiment("my_experiment_20231211")
print(f"Accuracy: {summary.average_scores.get('accuracy', 0):.3f}")
print(f"Completeness: {summary.average_scores.get('completeness', 0):.3f}")

# Comprehensive report oluÅŸtur
reports = reporter.generate_comprehensive_report([
    "experiment_1", 
    "experiment_2", 
    "experiment_3"
])

# Report dosyalarÄ±:
# - HTML: Interactive dashboard
# - CSV: Data analysis iÃ§in  
# - JSON: Programmatic access iÃ§in
""")


def demo_custom_evaluators():
    """Custom evaluator demo'su"""
    print("\nğŸ“š DEMO 4: Custom Evaluators")
    print("=" * 50)
    
    print("""
from langsmith.schemas import Run, Example
from langsmith.evaluation import evaluate

def netmera_specific_evaluator(run: Run, example: Example) -> dict:
    \"\"\"Netmera'ya Ã¶zel custom evaluator\"\"\"
    prediction = run.outputs.get("answer", "").lower()
    
    score = 0.0
    feedback = []
    
    # Netmera terimleri kontrolÃ¼
    netmera_terms = ["push notification", "segment", "campaign", "sdk"]
    used_terms = [term for term in netmera_terms if term in prediction]
    
    if used_terms:
        score += 0.4
        feedback.append(f"Netmera terimleri kullanÄ±ldÄ±: {used_terms}")
    
    # Kod Ã¶rneÄŸi kontrolÃ¼  
    if any(code_word in prediction for code_word in ["gradle", "json", "api", "method"]):
        score += 0.3
        feedback.append("Kod Ã¶rnekleri var")
    
    # Problem Ã§Ã¶zme odaklÄ±lÄ±k
    if any(solution in prediction for solution in ["Ã§Ã¶zÃ¼m", "solution", "fix", "adÄ±m"]):
        score += 0.3
        feedback.append("Problem Ã§Ã¶zme odaklÄ±")
    
    return {
        "key": "netmera_specific",
        "score": min(score, 1.0),
        "reason": "; ".join(feedback) if feedback else "Basic response"
    }

# Custom evaluator'Ä± kullan
results = evaluate(
    lambda inputs: evaluator.chatbot_predictor(inputs),
    data="netmera-developer-guide",
    evaluators=[netmera_specific_evaluator],
    experiment_prefix="custom_eval"
)
""")


def demo_advanced_analysis():
    """GeliÅŸmiÅŸ analiz demo'su"""
    print("\nğŸ“š DEMO 5: GeliÅŸmiÅŸ Analiz ve Monitoring")
    print("=" * 50)
    
    print("""
import pandas as pd
from src.evaluation.reporting import EvaluationReporter

# Reporter oluÅŸtur
reporter = EvaluationReporter()

# Son 30 gÃ¼nÃ¼n verilerini al
experiments = reporter.get_recent_experiments(30)
summaries = [reporter.analyze_experiment(exp) for exp in experiments]

# Performance trend analizi
df_data = []
for summary in summaries:
    df_data.append({
        'date': summary.timestamp,
        'experiment': summary.experiment_name,
        'dataset': summary.dataset_name,
        'accuracy': summary.average_scores.get('accuracy', 0),
        'completeness': summary.average_scores.get('completeness', 0),
        'helpfulness': summary.average_scores.get('helpfulness', 0)
    })

df = pd.DataFrame(df_data)

# HaftalÄ±k ortalama performance
weekly_avg = df.groupby(df['date'].dt.week)[['accuracy', 'completeness', 'helpfulness']].mean()
print("HaftalÄ±k performance trendi:")
print(weekly_avg)

# Dataset'e gÃ¶re performance karÅŸÄ±laÅŸtÄ±rmasÄ±
dataset_performance = df.groupby('dataset')[['accuracy', 'completeness', 'helpfulness']].mean()
print("\\nDataset performance karÅŸÄ±laÅŸtÄ±rmasÄ±:")
print(dataset_performance)

# Benchmark'larÄ±n altÄ±nda kalan alanlar
poor_performance = df[df['accuracy'] < 0.6][['experiment', 'dataset', 'accuracy']]
if not poor_performance.empty:
    print("\\nâš ï¸ Benchmark'Ä±n altÄ±ndaki performans:")
    print(poor_performance)
""")


def demo_automation():
    """Automation demo'su"""
    print("\nğŸ“š DEMO 6: Automated Evaluation Pipeline")
    print("=" * 50)
    
    print("""
# evaluation_pipeline.py
import schedule
import time
from datetime import datetime
from src.evaluation import NetmeraEvaluator, run_evaluation
from src.evaluation.reporting import EvaluationReporter

async def daily_evaluation():
    \"\"\"GÃ¼nlÃ¼k otomatik evaluation\"\"\"
    evaluator = NetmeraEvaluator()
    
    # TÃ¼m dataset'lerde evaluation Ã§alÄ±ÅŸtÄ±r
    datasets = ["netmera-developer-guide", "netmera-user-guide", 
               "netmera-developer-guide-en", "netmera-user-guide-en"]
    
    timestamp = datetime.now().strftime("%Y%m%d")
    
    for dataset in datasets:
        experiment_name = f"daily_eval_{dataset}_{timestamp}"
        
        try:
            results = await run_evaluation(evaluator, dataset, experiment_name)
            print(f"âœ… {dataset} evaluation completed")
        except Exception as e:
            print(f"âŒ {dataset} evaluation failed: {e}")
    
    # Report oluÅŸtur
    reporter = EvaluationReporter()
    recent_experiments = reporter.get_recent_experiments(1)
    
    if recent_experiments:
        reports = reporter.generate_comprehensive_report(recent_experiments)
        print(f"ğŸ“Š Daily report generated: {reports['html']}")

# Schedule daily evaluation
schedule.every().day.at("09:00").do(lambda: asyncio.run(daily_evaluation()))

# Run scheduler
while True:
    schedule.run_pending()
    time.sleep(3600)  # Check every hour
""")


def demo_ci_cd_integration():
    """CI/CD integration demo'su"""
    print("\nğŸ“š DEMO 7: CI/CD Integration")
    print("=" * 50)
    
    print("""
# .github/workflows/evaluation.yml
name: Chatbot Evaluation

on:
  push:
    branches: [ main ]
  schedule:
    - cron: '0 9 * * 1'  # Every Monday at 9 AM

jobs:
  evaluate:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Setup evaluation
      run: |
        python setup_evaluation.py
      env:
        LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
    
    - name: Run evaluation
      run: |
        python evaluate_chatbot.py --dataset all --experiment ci_cd_${{ github.sha }}
      env:
        LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
    
    - name: Generate reports
      run: |
        python -m src.evaluation.reporting --recent 1 --output ci_cd_report
      env:
        LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
    
    - name: Upload reports
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-reports
        path: evaluation_reports/
""")


def main():
    """Ana demo function"""
    print("ğŸ¯ Netmera Chatbot LangSmith Evaluation System")
    print("Demo ve KullanÄ±m Ã–rnekleri")
    print("=" * 60)
    
    # Check if setup was run
    if not Path("evaluation.env").exists() and not os.getenv("LANGSMITH_API_KEY"):
        print("âš ï¸  Setup henÃ¼z tamamlanmamÄ±ÅŸ!")
        print("   Ã–nce 'python setup_evaluation.py' Ã§alÄ±ÅŸtÄ±rÄ±n")
        print()
    
    demos = [
        ("Temel KullanÄ±m", demo_basic_usage),
        ("Python API", demo_python_api), 
        ("Reporting ve Analiz", demo_reporting),
        ("Custom Evaluators", demo_custom_evaluators),
        ("GeliÅŸmiÅŸ Analiz", demo_advanced_analysis),
        ("Automation Pipeline", demo_automation),
        ("CI/CD Integration", demo_ci_cd_integration)
    ]
    
    print("ğŸ“‹ Mevcut Demo'lar:")
    for i, (name, _) in enumerate(demos, 1):
        print(f"   {i}. {name}")
    
    print("\n" + "="*60)
    
    try:
        choice = input("Hangi demo'yu gÃ¶rmek istiyorsunuz? (1-7, 'all' for all, Enter for all): ").strip()
        
        if choice == "" or choice.lower() == "all":
            # TÃ¼m demo'larÄ± Ã§alÄ±ÅŸtÄ±r
            for name, demo_func in demos:
                demo_func()
        elif choice.isdigit() and 1 <= int(choice) <= len(demos):
            # Belirli demo'yu Ã§alÄ±ÅŸtÄ±r
            name, demo_func = demos[int(choice) - 1]
            demo_func()
        else:
            print("âŒ GeÃ§ersiz seÃ§im!")
            return
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Demo sonlandÄ±rÄ±ldÄ±")
        return
    
    print("\n" + "="*60)
    print("ğŸš€ Evaluation'Ä± baÅŸlatmak iÃ§in:")
    print("   python evaluate_chatbot.py --dataset all")
    print("\nğŸ“š DetaylÄ± dokÃ¼mantasyon:")
    print("   EVALUATION_README.md")
    print("\nğŸ”— LangSmith Dashboard:")
    print("   https://smith.langchain.com/")


if __name__ == "__main__":
    main()
